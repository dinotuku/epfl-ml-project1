{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "from implementations import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TRAIN_PATH = '../../../../data/train.csv'\n",
    "y, x, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../../../../data/test.csv'\n",
    "_, x_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The percentage of 's' label in y is 34.27%\n",
      "The percentage of 'b' label in y is 65.73%\n"
     ]
    }
   ],
   "source": [
    "# check the percentage of two labels\n",
    "print(\"The percentage of 's' label in y is {:.2f}%\".format(100 * y[y == 1].size / y.size))\n",
    "print(\"The percentage of 'b' label in y is {:.2f}%\".format(100 * y[y == -1].size / y.size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split training data into training and validation\n",
    "y_train, y_val, x_train, x_val = train_val_split(y, x, 0.2, seed=1)\n",
    "\n",
    "# normalize data using metrics of training data (except PRI_jet_num (22th column) since it is a discrete value)\n",
    "nor_indices = [idx for idx in range(x_train.shape[1]) if idx != 22]\n",
    "nor_x_train = x_train.copy()\n",
    "nor_x_val = x_val.copy()\n",
    "nor_x_test = x_test.copy()\n",
    "\n",
    "x_train_mean = x_train[:, nor_indices].mean(axis=0)\n",
    "x_train_std = x_train[:, nor_indices].std(axis=0)\n",
    "\n",
    "nor_x_train[:, nor_indices] = (nor_x_train[:, nor_indices] - x_train_mean) / x_train_std\n",
    "nor_x_val[:, nor_indices] = (nor_x_val[:, nor_indices] - x_train_mean) / x_train_std\n",
    "nor_x_test[:, nor_indices] = (nor_x_test[:, nor_indices] - x_train_mean) / x_train_std\n",
    "\n",
    "# add all ones column to features for bias term\n",
    "nor_x_train = np.c_[np.ones((nor_x_train.shape[0], 1)), nor_x_train]\n",
    "nor_x_val = np.c_[np.ones((nor_x_val.shape[0], 1)), nor_x_val]\n",
    "nor_x_test = np.c_[np.ones((nor_x_test.shape[0], 1)), nor_x_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression using gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear regression using gradient descent\n",
      "Gamma: 0.175\n",
      "Training Loss: 64032330817796781132584904233660117638476261848983926486010599406342868950761596823875325813899906662713931078847426461696.0000 - Training Accuracy: 0.3736\n",
      "Validation Loss: 84521039643156566896920776106210937900155729331145976058897769046159292625762787803258226382401015785142358805030304219136.0000 - Validation Accuracy: 0.3731\n",
      "Gamma: 0.15\n",
      "Training Loss: 0.3400 - Training Accuracy: 0.7446\n",
      "Validation Loss: 0.3400 - Validation Accuracy: 0.7441\n",
      "Gamma: 0.1\n",
      "Training Loss: 0.3400 - Training Accuracy: 0.7446\n",
      "Validation Loss: 0.3400 - Validation Accuracy: 0.7440\n"
     ]
    }
   ],
   "source": [
    "# set parameters\n",
    "max_iters = 1000\n",
    "initial_w = np.zeros(nor_x_train.shape[1])\n",
    "gammas = [0.175, 0.15, 0.1]\n",
    "\n",
    "# create history of weights and validation loss\n",
    "weights_history = []\n",
    "val_loss_history = []\n",
    "\n",
    "print('Linear regression using gradient descent')\n",
    "for gamma in gammas:\n",
    "    # train model, get weights and loss\n",
    "    weights, train_loss = least_squares_GD(y_train, nor_x_train, initial_w, max_iters, gamma)\n",
    "    val_loss = compute_ls_loss(y_val, nor_x_val, weights)\n",
    "\n",
    "    # make prediction\n",
    "    y_train_pred = predict_labels(weights, nor_x_train)\n",
    "    y_val_pred = predict_labels(weights, nor_x_val)\n",
    "\n",
    "    # compute accuracy\n",
    "    train_acc = compute_accuracy(y_train, y_train_pred)\n",
    "    val_acc = compute_accuracy(y_val, y_val_pred)\n",
    "    \n",
    "    # store weights and validation loss\n",
    "    weights_history.append(weights)\n",
    "    val_loss_history.append(val_loss)\n",
    "\n",
    "    print('Gamma:', gamma)\n",
    "    print(\"Training Loss: {:.4f} - Training Accuracy: {:.4f}\".format(train_loss, train_acc))\n",
    "    print(\"Validation Loss: {:.4f} - Validation Accuracy: {:.4f}\".format(val_loss, val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best gamma: 0.15\n"
     ]
    }
   ],
   "source": [
    "# get the best parameters\n",
    "best_idx = np.argmin(val_loss_history)\n",
    "best_gamma = gammas[best_idx]\n",
    "weights = weights_history[best_idx]\n",
    "print('Best gamma:', best_gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get prediction of testing data\n",
    "y_test_pred = predict_labels(weights, nor_x_test)\n",
    "OUTPUT_PATH = 'lr_gd.csv'\n",
    "create_csv_submission(ids_test, y_test_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression using stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear regression using stochastic gradient descent\n",
      "Gamma: 0.01\n",
      "Training Loss: 0.0718 - Training Accuracy: 0.6967\n",
      "Validation Loss: 0.3922 - Validation Accuracy: 0.6987\n",
      "Gamma: 0.005\n",
      "Training Loss: 1.4715 - Training Accuracy: 0.7132\n",
      "Validation Loss: 0.3632 - Validation Accuracy: 0.7144\n",
      "Gamma: 0.001\n",
      "Training Loss: 0.0663 - Training Accuracy: 0.7166\n",
      "Validation Loss: 0.3659 - Validation Accuracy: 0.7171\n"
     ]
    }
   ],
   "source": [
    "# set parameters\n",
    "max_iters = 1000\n",
    "initial_w = np.zeros(nor_x_train.shape[1])\n",
    "gammas = [0.01, 0.005, 0.001]\n",
    "\n",
    "# create history of weights and validation loss\n",
    "weights_history = []\n",
    "val_loss_history = []\n",
    "\n",
    "print('Linear regression using stochastic gradient descent')\n",
    "for gamma in gammas:\n",
    "    # train model, get weights and loss\n",
    "    weights, train_loss = least_squares_SGD(y_train, nor_x_train, initial_w, max_iters, gamma)\n",
    "    val_loss = compute_ls_loss(y_val, nor_x_val, weights)\n",
    "\n",
    "    # make prediction\n",
    "    y_train_pred = predict_labels(weights, nor_x_train)\n",
    "    y_val_pred = predict_labels(weights, nor_x_val)\n",
    "\n",
    "    # compute accuracy\n",
    "    train_acc = compute_accuracy(y_train, y_train_pred)\n",
    "    val_acc = compute_accuracy(y_val, y_val_pred)\n",
    "    \n",
    "    # store weights and validation loss\n",
    "    weights_history.append(weights)\n",
    "    val_loss_history.append(val_loss)\n",
    "\n",
    "    print('Gamma:', gamma)\n",
    "    print(\"Training Loss: {:.4f} - Training Accuracy: {:.4f}\".format(train_loss, train_acc))\n",
    "    print(\"Validation Loss: {:.4f} - Validation Accuracy: {:.4f}\".format(val_loss, val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best gamma: 0.005\n"
     ]
    }
   ],
   "source": [
    "# get the best parameters\n",
    "best_idx = np.argmin(val_loss_history)\n",
    "best_gamma = gammas[best_idx]\n",
    "weights = weights_history[best_idx]\n",
    "print('Best gamma:', best_gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get prediction of testing data\n",
    "y_test_pred = predict_labels(weights, nor_x_test)\n",
    "OUTPUT_PATH = 'lr_sgd.csv'\n",
    "create_csv_submission(ids_test, y_test_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least squares regression using normal equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Least squares regression using normal equations\n",
      "Training Loss: 0.3395 - Training Accuracy: 0.7452\n",
      "Validation Loss: 0.3394 - Validation Accuracy: 0.7445\n"
     ]
    }
   ],
   "source": [
    "# train model, get weights and loss\n",
    "weights, train_loss = least_squares(y_train, nor_x_train)\n",
    "val_loss = compute_ls_loss(y_val, nor_x_val, weights)\n",
    "\n",
    "# make prediction\n",
    "y_train_pred = predict_labels(weights, nor_x_train)\n",
    "y_val_pred = predict_labels(weights, nor_x_val)\n",
    "\n",
    "# compute accuracy\n",
    "train_acc = compute_accuracy(y_train, y_train_pred)\n",
    "val_acc = compute_accuracy(y_val, y_val_pred)\n",
    "\n",
    "print('Least squares regression using normal equations')\n",
    "print(\"Training Loss: {:.4f} - Training Accuracy: {:.4f}\".format(train_loss, train_acc))\n",
    "print(\"Validation Loss: {:.4f} - Validation Accuracy: {:.4f}\".format(val_loss, val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get prediction of testing data\n",
    "y_test_pred = predict_labels(weights, nor_x_test)\n",
    "OUTPUT_PATH = 'ls.csv'\n",
    "create_csv_submission(ids_test, y_test_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge regression using normal equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge regression using normal equations\n",
      "Lambda: 0.001\n",
      "Training Loss: 0.3400 - Training Accuracy: 0.7445\n",
      "Validation Loss: 0.3400 - Validation Accuracy: 0.7436\n",
      "Lambda: 0.0005\n",
      "Training Loss: 0.3400 - Training Accuracy: 0.7447\n",
      "Validation Loss: 0.3400 - Validation Accuracy: 0.7438\n",
      "Lambda: 0.0001\n",
      "Training Loss: 0.3400 - Training Accuracy: 0.7447\n",
      "Validation Loss: 0.3400 - Validation Accuracy: 0.7439\n"
     ]
    }
   ],
   "source": [
    "# set parameters\n",
    "lambdas = [0.001, 0.0005, 0.0001]\n",
    "\n",
    "# create history of weights and validation loss\n",
    "weights_history = []\n",
    "val_loss_history = []\n",
    "\n",
    "print('Ridge regression using normal equations')\n",
    "for lambda_ in lambdas:\n",
    "    # train model, get weights and loss\n",
    "    weights, train_loss = ridge_regression(y_train, nor_x_train, lambda_)\n",
    "    val_loss = compute_ls_loss(y_val, nor_x_val, weights)\n",
    "\n",
    "    # make prediction\n",
    "    y_train_pred = predict_labels(weights, nor_x_train)\n",
    "    y_val_pred = predict_labels(weights, nor_x_val)\n",
    "\n",
    "    # compute accuracy\n",
    "    train_acc = compute_accuracy(y_train, y_train_pred)\n",
    "    val_acc = compute_accuracy(y_val, y_val_pred)\n",
    "    \n",
    "    # store weights and validation loss\n",
    "    weights_history.append(weights)\n",
    "    val_loss_history.append(val_loss)\n",
    "\n",
    "    print('Lambda:', lambda_)\n",
    "    print(\"Training Loss: {:.4f} - Training Accuracy: {:.4f}\".format(train_loss, train_acc))\n",
    "    print(\"Validation Loss: {:.4f} - Validation Accuracy: {:.4f}\".format(val_loss, val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best lambda: 0.0001\n"
     ]
    }
   ],
   "source": [
    "# get the best parameters\n",
    "best_idx = np.argmin(val_loss_history)\n",
    "best_lambda_ = lambdas[best_idx]\n",
    "weights = weights_history[best_idx]\n",
    "print('Best lambda:', best_lambda_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get prediction of testing data\n",
    "y_test_pred = predict_labels(weights, nor_x_test)\n",
    "OUTPUT_PATH = 'rr.csv'\n",
    "create_csv_submission(ids_test, y_test_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression using gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change y label from (1, -1) to (1, 0)\n",
    "lg_y_train = np.where(y_train == -1, 0, y_train)\n",
    "lg_y_val = np.where(y_val == -1, 0, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression using gradient descent\n",
      "Gamma: 1\n",
      "Training Loss: 0.3400 - Training Accuracy: 0.7507\n",
      "Validation Loss: 0.4979 - Validation Accuracy: 0.7495\n",
      "Gamma: 0.75\n",
      "Training Loss: 0.3400 - Training Accuracy: 0.7507\n",
      "Validation Loss: 0.4979 - Validation Accuracy: 0.7496\n",
      "Gamma: 0.5\n",
      "Training Loss: 0.3400 - Training Accuracy: 0.7507\n",
      "Validation Loss: 0.4980 - Validation Accuracy: 0.7497\n"
     ]
    }
   ],
   "source": [
    "# set parameters\n",
    "initial_w = np.zeros(nor_x_train.shape[1])\n",
    "max_iters = 1000\n",
    "gammas = [1, 0.75, 0.5]\n",
    "\n",
    "# create history of weights and validation loss\n",
    "weights_history = []\n",
    "val_loss_history = []\n",
    "\n",
    "print('Logistic regression using gradient descent')\n",
    "for gamma in gammas:\n",
    "    # train model, get weights and loss\n",
    "    weights, loss = logistic_regression(lg_y_train, nor_x_train, initial_w, max_iters, gamma)\n",
    "    val_loss = compute_lg_loss(lg_y_val, nor_x_val, weights)\n",
    "\n",
    "    # make prediction\n",
    "    y_train_pred = predict_lg_labels(weights, nor_x_train)\n",
    "    y_val_pred = predict_lg_labels(weights, nor_x_val)\n",
    "\n",
    "    # compute accuracy\n",
    "    train_acc = compute_accuracy(lg_y_train, y_train_pred)\n",
    "    val_acc = compute_accuracy(lg_y_val, y_val_pred)\n",
    "    \n",
    "    # store weights and validation loss\n",
    "    weights_history.append(weights)\n",
    "    val_loss_history.append(val_loss)\n",
    "\n",
    "    print('Gamma:', gamma)\n",
    "    print(\"Training Loss: {:.4f} - Training Accuracy: {:.4f}\".format(train_loss, train_acc))\n",
    "    print(\"Validation Loss: {:.4f} - Validation Accuracy: {:.4f}\".format(val_loss, val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best gamma: 1\n"
     ]
    }
   ],
   "source": [
    "# get the best parameters\n",
    "best_idx = np.argmin(val_loss_history)\n",
    "best_gamma = gammas[best_idx]\n",
    "weights = weights_history[best_idx]\n",
    "print('Best gamma:', best_gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get prediction of testing data\n",
    "y_test_pred = predict_lg_labels(weights, nor_x_test)\n",
    "y_test_pred[y_test_pred == 0] = -1\n",
    "OUTPUT_PATH = 'lg_gd.csv'\n",
    "create_csv_submission(ids_test, y_test_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized logistic regression using gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression using gradient descent\n",
      "Lambda: 0.01 - Gamma: 1\n",
      "Training Loss: 0.3400 - Training Accuracy: 0.7310\n",
      "Validation Loss: 0.5432 - Validation Accuracy: 0.7299\n",
      "Lambda: 0.01 - Gamma: 0.75\n",
      "Training Loss: 0.3400 - Training Accuracy: 0.7467\n",
      "Validation Loss: 0.5056 - Validation Accuracy: 0.7463\n",
      "Lambda: 0.01 - Gamma: 0.5\n",
      "Training Loss: 0.3400 - Training Accuracy: 0.7467\n",
      "Validation Loss: 0.5056 - Validation Accuracy: 0.7464\n",
      "Lambda: 0.005 - Gamma: 1\n",
      "Training Loss: 0.3400 - Training Accuracy: 0.7431\n",
      "Validation Loss: 0.5195 - Validation Accuracy: 0.7411\n",
      "Lambda: 0.005 - Gamma: 0.75\n",
      "Training Loss: 0.3400 - Training Accuracy: 0.7486\n",
      "Validation Loss: 0.5014 - Validation Accuracy: 0.7470\n",
      "Lambda: 0.005 - Gamma: 0.5\n",
      "Training Loss: 0.3400 - Training Accuracy: 0.7486\n",
      "Validation Loss: 0.5014 - Validation Accuracy: 0.7471\n",
      "Lambda: 0.001 - Gamma: 1\n",
      "Training Loss: 0.3400 - Training Accuracy: 0.7503\n",
      "Validation Loss: 0.4982 - Validation Accuracy: 0.7489\n",
      "Lambda: 0.001 - Gamma: 0.75\n",
      "Training Loss: 0.3400 - Training Accuracy: 0.7502\n",
      "Validation Loss: 0.4982 - Validation Accuracy: 0.7489\n",
      "Lambda: 0.001 - Gamma: 0.5\n",
      "Training Loss: 0.3400 - Training Accuracy: 0.7502\n",
      "Validation Loss: 0.4983 - Validation Accuracy: 0.7489\n"
     ]
    }
   ],
   "source": [
    "# set parameters\n",
    "lambdas = [0.01, 0.005, 0.001]\n",
    "initial_w = np.zeros(nor_x_train.shape[1])\n",
    "max_iters = 1000\n",
    "gammas = [1, 0.75, 0.5]\n",
    "\n",
    "# create history of weights and validation loss\n",
    "weights_history = []\n",
    "val_loss_history = []\n",
    "\n",
    "print('Logistic regression using gradient descent')\n",
    "for lambda_ in lambdas:\n",
    "    for gamma in gammas:\n",
    "        # train model, get weights and loss\n",
    "        weights, loss = reg_logistic_regression(lg_y_train, nor_x_train, lambda_, initial_w, max_iters, gamma)\n",
    "        val_loss = compute_lg_loss(lg_y_val, nor_x_val, weights)\n",
    "\n",
    "        # make prediction\n",
    "        y_train_pred = predict_lg_labels(weights, nor_x_train)\n",
    "        y_val_pred = predict_lg_labels(weights, nor_x_val)\n",
    "\n",
    "        # compute accuracy\n",
    "        train_acc = compute_accuracy(lg_y_train, y_train_pred)\n",
    "        val_acc = compute_accuracy(lg_y_val, y_val_pred)\n",
    "        \n",
    "        # store weights and validation loss\n",
    "        weights_history.append(weights)\n",
    "        val_loss_history.append(val_loss)\n",
    "\n",
    "        print(\"Lambda: {} - Gamma: {}\".format(lambda_, gamma))\n",
    "        print(\"Training Loss: {:.4f} - Training Accuracy: {:.4f}\".format(train_loss, train_acc))\n",
    "        print(\"Validation Loss: {:.4f} - Validation Accuracy: {:.4f}\".format(val_loss, val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best lambda: 0.001 - Best gamma: 1\n"
     ]
    }
   ],
   "source": [
    "# get the best parameters\n",
    "best_idx = np.argmin(val_loss_history)\n",
    "best_lambda_ = lambdas[int(best_idx / len(lambdas))]\n",
    "best_gamma = gammas[best_idx % len(gammas)]\n",
    "weights = weights_history[best_idx]\n",
    "print(\"Best lambda: {} - Best gamma: {}\".format(best_lambda_, best_gamma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get prediction of testing data\n",
    "y_test_pred = predict_lg_labels(weights, nor_x_test)\n",
    "y_test_pred[y_test_pred == 0] = -1\n",
    "OUTPUT_PATH = 'lg_sgd.csv'\n",
    "create_csv_submission(ids_test, y_test_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200000, 30)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAELCAYAAAAoUKpTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAH1BJREFUeJzt3X+cVXW97/HX+wyRKXawogwG+eHoiD9wyMG0HqbiVYgM76O4OnjvUbDiVEL3cNR77ZFyuHROcdKcR4Z6ruWPMpvRyBQNNU9mh2MYDIo/ABFkRAYqIfEaZSL4uX/sNbiZ2XvWZpg1s2fm/Xw89sP9/a7vWvvzdc/Mh/Vda32/igjMzMw68jc9HYCZmZU/JwszM0vlZGFmZqmcLMzMLJWThZmZpXKyMDOzVJkmC0mTJK2TtEHSlQW210talbxekPRa3rYjJP1C0lpJaySNzDJWMzMrTlk9ZyGpAngBOBtoAVYA0yJiTZH2s4FxEXFJUn4M+JeIeETSIODtiPhLJsGamVmHsjyzOBnYEBEbI2IX0Aic10H7aUADgKRjgQER8QhAROx0ojAz6zkDMjz2MGBzXrkF+GihhpJGAKOAR5Oqo4HXJN2T1P87cGVE7Gmz30xgJsAhhxxy0jHHHNOlHTAz6+tWrly5PSKGpLXLMlmoQF2xMa86YFFeMhgAnAaMA14G7gKmA7fsc7CIm4GbAWpra6OpqenAozYz60ckbSqlXZbDUC3A8LxyJbC1SNs6kiGovH2fSoawdgP3Ah/JJEozM0uVZbJYARwlaZSkgeQSwuK2jSRVA4cBy9rse5ik1lOjCUDBC+NmZpa9zJJFckYwC3gYWAvcHRGrJc2XNCWv6TSgMfJuy0qGoy4HfinpWXJDWt/LKlYzM+tYZrfOdjdfszAz23+SVkZEbVo7P8FtZmapnCzMzCyVk4WZmaVysjAzs1ROFmZmlsrJwszMUjlZmJlZKicLMzNL5WRhZmapnCzMzCyVk4WZmaVysjAzs1ROFmZmlsrJwsy61UMPPUR1dTVVVVUsWLCg3fY5c+ZQU1NDTU0NRx99NIMHD967raKiYu+2KVPeWelg4cKFVFVVIYnt27d3Sz/y9cU+tRMRfeJ10kknhZmVt927d8fo0aPjxRdfjDfffDPGjh0bq1evLtr++uuvjxkzZuwtH3LIIQXbPfnkk9Hc3BwjRoyIbdu2dXncHentfQKaooS/sT6zMLNus3z5cqqqqhg9ejQDBw6krq6O++67r2j7hoYGpk2blnrccePGMXLkyC6MtHR9sU+FOFmYWbfZsmULw4cP31uurKxky5YtBdtu2rSJ5uZmJkyYsLfur3/9K7W1tZxyyince++9mcdbir7Yp0IG9HQAZtZ/RIGVOSUVbNvY2MjUqVOpqKjYW/fyyy8zdOhQNm7cyIQJEzjhhBM48sgjM4u3FH2xT4X4zMLMuk1lZSWbN2/eW25paWHo0KEF2zY2NrYbrmltO3r0aM444wyeeuqp7IItUV/sUyFOFmbWbcaPH8/69etpbm5m165dNDY27nMHUKt169axY8cOTj311L11O3bs4M033wRg+/btPP744xx77LHdFnsxfbFPhThZmFm3GTBgAAsXLmTixImMGTOG888/n+OOO465c+eyePHive0aGhqoq6vbZzhn7dq11NbWcuKJJ3LmmWdy5ZVX7v3Dev3111NZWUlLSwtjx47l85//vPvUxVRovK03qq2tjaampp4Ow8ysV5G0MiJq09r5zMLMzFI5WZiZWSonCzMzS+VkYWZmqZwszMwsVaZPcEuaBHwHqAC+HxEL2myvB85MigcDH4yIwcm2PcCzybaXI6L9jctmVrbmX/ZAT4dQ0Nxvn3tA+y/86owuiqTrzPrmbZl/RmbJQlIFcANwNtACrJC0OCLWtLaJiDl57WcD4/IO8UZE1GQVn5mZlS7LYaiTgQ0RsTEidgGNwHkdtJ8GNGQYj5mZdVKWyWIYsDmv3JLUtSNpBDAKeDSv+iBJTZKekPRfswvTzMzSZHnNotC0i8UeF68DFkXEnry6IyJiq6TRwKOSno2IF/f5AGkmMBPgiCOO6IqYzcysgCzPLFqA4XnlSmBrkbZ1tBmCioityX83Ao+x7/WM1jY3R0RtRNQOGTKkK2I2M7MCskwWK4CjJI2SNJBcQljctpGkauAwYFle3WGS3p28/wDwcWBN233NzKx7ZJYsImI3MAt4GFgL3B0RqyXNl5R/G+w0oDH2ndFwDNAk6WngV8CC/LuozNp66KGHqK6upqqqigULFrTbPmfOHGpqaqipqeHoo49m8ODB+2x//fXXGTZsGLNmzdpbd8YZZ1BdXb13v1deeSXzfuTri32y3ivT5ywiYgmwpE3d3DbleQX2+w1wQpaxWd+xZ88eLr30Uh555BEqKysZP348U6ZM2WddgPr6+r3vv/vd77ZbYObqq6/m9NNPb3fsO++8k9ra1Ak5u1xf7JP1bn6C23q95cuXU1VVxejRoxk4cCB1dXXcd999Rds3NDTss1rZypUr+cMf/sA555zTHeGWpC/2yXo3Jwvr9bZs2cLw4e/cS1FZWcmWLVsKtt20aRPNzc1MmDABgLfffpvLLruMa665pmD7GTNmUFNTw9e//vWCay1npS/2yXo3J4sOZDFm3GrKlCkcf/zxmcXenxT6g5e/Glm+xsZGpk6dSkVFBQA33ngjkydP3ucPc6s777yTZ599lqVLl7J06VLuuOOOrg28A32xT9a7ZXrNojfLcsz4nnvuYdCgQdkF389UVlayefM7z3+2tLQwdOjQgm0bGxu54YYb9paXLVvG0qVLufHGG9m5cye7du1i0KBBLFiwgGHDcs+QHnrooVx44YUsX76ciy66KNvOJPpin6x385lFEVmNGe/cuZPrrruOq666KrPY+5vx48ezfv16mpub2bVrF42NjUyZ0n7eyXXr1rFjxw5OPfXUvXV33nknL7/8Mi+99BLXXnstF110EQsWLGD37t1s374dgLfeeosHHnigW88E+2KfrHdzsigiqzHjq6++mssuu4yDDz44m8D7oQEDBrBw4UImTpzImDFjOP/88znuuOOYO3cuixe/82hPQ0MDdXV1RYdz8r355ptMnDiRsWPHUlNTw7Bhw/jCF76QZTf20Rf7ZL2bh6GKyGLMeNWqVWzYsIH6+npeeumlLo+5P5s8eTKTJ0/ep27+/Pn7lOfNm9fhMaZPn8706dMBOOSQQ1i5cmVXhrjf+mKfrPdysigiizHjESNGsHLlSkaOHMnu3bt55ZVXOOOMM3jsscey7o6Z2QFxsigif8x42LBhNDY28uMf/7hdu2Jjxq1uv/12mpqa9t5N9aUvfQmAl156iXPPPdeJwsx6BV+zKCKLMWMzs95KfeWhnNra2mhqaurpMMws4WVVu8+BLKsqaWVEpM7/4jMLMzNL5WRhZmapfIHbepXz7/pST4dQ0N0X3HRA+z9+3me7KJKu8/H7ftrTIVgZ8ZmFmZmlcrIwM7NUHoZKfPqy4vM+9ZT7v31eT4dgZgb4zMLMzErgZGFmZqmcLMzMLJWThZmZpXKyMDOzVE4WZmaWysnCzMxSOVmYmVkqJwszM0vlZGFmZqmcLMzMLFWmyULSJEnrJG2QdGWB7fWSViWvFyS91mb7eyVtkbQwyzjNzKxjmU0kKKkCuAE4G2gBVkhaHBFrWttExJy89rOBcW0O83Xg11nFaGZmpcnyzOJkYENEbIyIXUAj0NE0qtOAhtaCpJOADwG/yDBGMzMrQZbJYhiwOa/cktS1I2kEMAp4NCn/DfBt4IqOPkDSTElNkpq2bdvWJUGbmVl7WSYLFaiLIm3rgEURsScpfxlYEhGbi7TPHSzi5oiojYjaIUOGHECoZmbWkSwXP2oBhueVK4GtRdrWAZfmlU8FTpP0ZWAQMFDSzohod5HczMyyl2WyWAEcJWkUsIVcQriwbSNJ1cBhwLLWuoj473nbpwO1ThRmZj0ns2GoiNgNzAIeBtYCd0fEaknzJU3JazoNaIyIYkNUZmbWwzJdgzsilgBL2tTNbVOel3KM24Hbuzg0MzPbD36C28zMUjlZmJlZKicLMzNL5WRhZmapnCzMzCyVk4WZmaVysjAzs1ROFmZmlsrJwszMUjlZmJlZqtRkIWmWpMO6IxgzMytPpZxZHE5uSdS7kzW1C61TYWZmfVhqsoiIq4CjgFuA6cB6Sd+QdGTGsZmZWZko6ZpFMn3475PXbnLrTyyS9K0MYzMzszKROkW5pK8AFwPbge8DV0TEW8k62euB/5VtiGZm1tNKWc/iA8BnImJTfmVEvC3p3GzCMjOzclLKMNQS4NXWgqRDJX0UICLWZhWYmZmVj1KSxU3Azrzyn5M6MzPrJ0pJFspfHzsi3ibj5VjNzKy8lJIsNkr6iqR3Ja//CWzMOjAzMysfpSSLLwIfA7YALcBHgZlZBmVmZuUldTgpIl4B6rohFjMzK1OlPGdxEPA54DjgoNb6iLgkw7jMzKyMlDIMdQe5+aEmAr8GKoE/ZRmUmZmVl1KSRVVEXA38OSJ+AHwKOCHbsMzMrJyUkizeSv77mqTjgb8FRmYWkZmZlZ1Snpe4OVnP4ipgMTAIuDrTqMzMrKx0mCySyQJfj4gdwH8Ao/fn4JImAd8BKoDvR8SCNtvrgTOT4sHAByNisKQRwD3Jfu8CvhsR/7Y/n21mZl2nw2SRTBY4C7h7fw8sqQK4ATib3PMZKyQtjog1ecefk9d+NjAuKf4O+FhEvClpEPBcsu/W/Y3DzMwOXCnXLB6RdLmk4ZLe1/oqYb+TgQ0RsTEidgGNwHkdtJ8GNABExK6IeDOpf3eJcZqZWUZKuWbR+jzFpXl1QfqQ1DBgc1659envdpJhp1HAo3l1w4GfA1Xk1tBod1YhaSbJ0+RHHHFESjhmZtZZpTzBPaqTxy60VncUqIPcE+KLImJP3uduBsZKGgrcK2lRRPyhTWw3AzcD1NbWFju2mZkdoFKe4L6oUH1E/DBl1xZgeF65Eih2zaGOfc9c8j9nq6TVwGnAopTPNDOzDJQyDDU+7/1BwFnAk0BaslgBHCVpFLlJCOuAC9s2klRNbk3vZXl1lcAfI+KN5LbdjwPXlRCrmZlloJRhqNn5ZUl/S24KkLT9did3Uj1M7hbYWyNitaT5QFNELE6aTgMa89fMAMYA35YU5Iazro2IZ0vqkZmZdbnOLGL0F+CoUhpGxBJyy7Lm181tU55XYL9HgLGdiM3MzDJQyjWL+3nnwvTfAMfSiecuzMys9yrlzOLavPe7gU0R0ZJRPGZmVoZKSRYvA7+LiL8CSHqPpJER8VKmkZmZWdko5cnonwBv55X3JHVmZtZPlJIsBiTTdQC5qTiAgdmFZGZm5aaUZLFN0pTWgqTzgO3ZhWRmZuWmlGsWXwTulLQwKbcABZ/qNjOzvqmUh/JeBE5JpgpXRHj9bTOzfiZ1GErSNyQNjoidEfEnSYdJ+ufuCM7MzMpDKdcsPhkRr7UWklXzJmcXkpmZlZtSkkWFpHe3FiS9h9yCRGZm1k+UcoH7R8AvJd2WlGcAP8guJDMzKzelXOD+lqRngP9CbgbYh4ARWQdmZmblo9S1rX9P7inuz5Jbz2JtZhGZmVnZKXpmIelocgsWTQP+CNxF7tbZM7spNjMzKxMdDUM9DywFPh0RGwAkzemWqMzMrKx0NAz1WXLDT7+S9D1JZ5G7ZmFmZv1M0WQRET+LiAuAY4DHgDnAhyTdJOmcborPzMzKQOoF7oj4c0TcGRHnApXAKuDKzCMzM7OyUerdUABExKsR8X8jYkJWAZmZWfnZr2RhZmb9k5OFmZmlcrIwM7NUThZmZpbKycLMzFI5WZiZWSonCzMzS5VpspA0SdI6SRsktXuQT1K9pFXJ6wVJryX1NZKWSVot6RlJF2QZp5mZdayUxY86RVIFcANwNtACrJC0OCLWtLaJiDl57WcD45LiX4CLImK9pKHASkkP5y/vamZm3SfLM4uTgQ0RsTEidgGNwHkdtJ8GNABExAsRsT55vxV4BRiSYaxmZtaBLJPFMGBzXrklqWtH0ghgFPBogW0nAwOBFwtsmympSVLTtm3buiRoMzNrL8tkUWg68yjStg5YFBF79jmA9GHgDmBGRLzd7mARN0dEbUTUDhniEw8zs6xkmSxagOF55Upga5G2dSRDUK0kvRf4OXBVRDyRSYRmZlaSLJPFCuAoSaMkDSSXEBa3bSSpGjgMWJZXNxD4GfDDiPhJhjGamVkJMksWEbEbmAU8DKwF7o6I1ZLmS5qS13Qa0BgR+UNU5wOfAKbn3Vpbk1WsZmbWscxunQWIiCXAkjZ1c9uU5xXY70fAj7KMzczMSucnuM3MLJWThZmZpXKyMDOzVE4WZmaWysnCzMxSOVn0Mw899BDV1dVUVVWxYMGCdtvnzJlDTU0NNTU1HH300QwePHjvtkmTJjF48GDOPffcffb53Oc+x4knnsjYsWOZOnUqO3fuzLwfZta9nCz6kT179nDppZfy4IMPsmbNGhoaGlizZs0+berr61m1ahWrVq1i9uzZfOYzn9m77YorruCOO+5od9z6+nqefvppnnnmGY444ggWLlyYeV/MrHs5WfQjy5cvp6qqitGjRzNw4EDq6uq47777irZvaGhg2rRpe8tnnXUWhx56aLt2733vewGICN544w2kQtOCmVlv5mTRj2zZsoXhw9+ZrquyspItW7YUbLtp0yaam5uZMGFCSceeMWMGhx9+OM8//zyzZ8/uknjNrHw4WfQj+86oklPsLKCxsZGpU6dSUVFR0rFvu+02tm7dypgxY7jrrrsOKE4zKz9OFv1IZWUlmze/s8RIS0sLQ4cOLdi2sbFxnyGoUlRUVHDBBRfw05/+9IDiNLPy42TRj4wfP57169fT3NzMrl27aGxsZMqUKe3arVu3jh07dnDqqaemHjMi2LBhw973999/P8ccc0yXx25mPSvTiQStvAwYMICFCxcyceJE9uzZwyWXXMJxxx3H3Llzqa2t3Zs4GhoaqKurazdEddppp/H888+zc+dOKisrueWWWzj77LO5+OKLef3114kITjzxRG666aae6J6ZZcjJop+ZPHkykydP3qdu/vz5+5TnzZtXcN+lS5cWrH/88ce7JDYzK18ehjIzs1ROFmZmlsrJwszMUjlZmJlZKicLMzNL5WRhZmapfOtsH7bxXz7b0yG0M/prfrrbrDfymYWZmaVysjAzs1ROFmZmlsrJwszMUjlZmJlZKicLMzNLlWmykDRJ0jpJGyRdWWB7vaRVyesFSa/lbXtI0muSHsgyRjMzS5fZcxaSKoAbgLOBFmCFpMURsaa1TUTMyWs/GxiXd4hrgIOBv88qRjMzK02WZxYnAxsiYmNE7AIagfM6aD8NaGgtRMQvgT9lGJ+ZmZUoy2QxDNicV25J6tqRNAIYBTyaYTxmZtZJWSYLFaiLIm3rgEURsWe/PkCaKalJUtO2bdv2O0AzMytNlsmiBRieV64EthZpW0feEFSpIuLmiKiNiNohQ4Z0IkQzMytFlsliBXCUpFGSBpJLCIvbNpJUDRwGLMswFjMzOwCZJYuI2A3MAh4G1gJ3R8RqSfMlTclrOg1ojIh9hqgkLQV+ApwlqUXSxKxiNTOzjmU6RXlELAGWtKmb26Y8r8i+p2UXmZmZ7Q8/wW1mZqmcLMzMLJWThZmZpXKyMDOzVE4WZmaWysnCzMxSOVmYmVkqJwszM0vlZGFmZqmcLMzMLJWThZmZpXKyMDOzVE4WZmaWysnCzMxSOVmYmVkqJwszM0vlZGFmZqmcLMzMLJWThZmZpXKyMDOzVE4WZmaWysnCzMxSOVmYmVkqJwszM0vlZGFmZqmcLMzMLJWThZmZpXKyMDOzVJkmC0mTJK2TtEHSlQW210talbxekPRa3raLJa1PXhdnGaeZmXVsQFYHllQB3ACcDbQAKyQtjog1rW0iYk5e+9nAuOT9+4B/AmqBAFYm++7IKl4zMysuyzOLk4ENEbExInYBjcB5HbSfBjQk7ycCj0TEq0mCeASYlGGsZmbWAUVENgeWpgKTIuLzSfnvgI9GxKwCbUcATwCVEbFH0uXAQRHxz8n2q4E3IuLaNvvNBGYmxWpgXSad2T8fALb3dBAZ6Iv96ot9gr7ZL/cpOyMiYkhao8yGoQAVqCuWmeqARRGxZ3/2jYibgZs7F142JDVFRG1Px9HV+mK/+mKfoG/2y33qeVkOQ7UAw/PKlcDWIm3reGcIan/3NTOzjGWZLFYAR0kaJWkguYSwuG0jSdXAYcCyvOqHgXMkHSbpMOCcpM7MzHpAZsNQEbFb0ixyf+QrgFsjYrWk+UBTRLQmjmlAY+RdPImIVyV9nVzCAZgfEa9mFWsXK6thsS7UF/vVF/sEfbNf7lMPy+wCt5mZ9R1+gtvMzFI5WZiZWSonC+sXJO0sUFct6bFkupm1knrXGLK0J4n9OUn3Sxqc1I+U9EaybY2kH0p6V0/HW4pC31NS/z8kPSNptaSnJX2/tb/lKOW7ea7IPv8o6XlJzyZ9vK6cvjcniyKK/HGZJ2lL3i/htJRjnCLpt3l/jOYl9dMlbZP0VDL31cOSPtab+pBsmyRpefIDvkrSXZKOSLbdLqk5+aF/IfmDNayr+3iArgfqI6ImIsYA3+3pgPbTG0nsxwOvApfmbXsxImqAE8jden5+TwTYFSRNAuYAn4yI44CPAL8BPtSjgXWso++mHUlfJHfX5ykRcQIwHngFeE/mkZYoy4fy+qr6iLhW0lHk5qxaFBFvFWn7A+D8iHg6mSurOm/bXa1Ps0s6E7hH0pkRsTbb8IEu6IOk48n9cZ3SGrOkKcBI4OVk3ysiYpEkAf8A/ErS8cn0L+Xgw+Se6QEgIp7twVgO1DJgbNvKZEaE5UC5Jer98TXg8ojYArk+Abf2bEj7peB308bXgE9ExGsAye/IgqwD2x8+s+ikiFgP/IXcMyLFfBD4XdJ+T/4kim2O9Styt9HNLLQ9KwfYh/8NfCM/uUXE4oj4jwKfExFRD/we+GRXxd8F6oFHJT0oaU45D2t0JEniZ1H4OaaDgI8CD3V3XF3oOODJng6iMzr6bvLaHAoMiojmbgusE5wsOknSR4D1EfFKB83qgXWSfibp75Nf3GKeBI7p0iBTHGAfOvML3O197EhE3AaMAX4CnAE8IendPRrU/nmPpFXAH4H3kZtws9WRedtejohneiLAribphGTI80VJF/R0PB3o6LtpS+RNZyRpYtLHl7IYnu4sJ4v9N0fSOuC3wLyOGkbEfHLTrP8CuJCO/3VXaD6srHRpHyS9X++sSXJ5B4frzj6WJCK2RsStEXEesBs4vqdj2g9vJNclRgADKXzNogo4JRki7K1Wk7tOQUQ8m/TrQcpoPL+Ajr6bfUTE68CfJY1Kyg8n+z6X7FsWnCz2X31EVAMXAD9MOVsgIl6MiJvInYqeKOn9RZqOA7rjegV0TR/yf4H/mPxw3wwM6uBQ3dnHVMkF+ncl7w8H3g9s6dmo9l9E/D/gK8Dlbe+eiYjfAVcCX+2J2LrIN4FrJVXm1ZVzotiro++mjW8CN+XdNSWgw9/L7uZk0UkRcQ/QBBRdxU/Sp5IvHeAoYA/wWoF2p5O7XvG9DEIt6gD78C3ga5LG5DU/uMgxJOkr5C4o99TY+cGSWvJe/0ju7pPnJD1NblqaKyLi9z0U3wGJiKeAp8nNwdbWveT6f1r3RtUp7b6niFhC7s61B5M7+H5D7uewV8wXV+C7qW7Tx/8G3AT8O/BbSc8AjwNPJa+y4Ok+ipD0NvvOdHsd8F5gZ+u6GpJOAn4MjImItwsco5Hcv77/Qm6I42sR8bCk6cA15P4VezDQTG7+q8d7Sx+SbZ8iN4x1KMnYOPBPEfGCpNuB04HXkz4+AXw1IlrafoaZlT8nCzMzS+VhKDMzS+WH8rqApBuAj7ep/k5ya2av0Bf6YGbZ8TCUmZml8jCUmZmlcrIwM7NUThbW70gKSXfklQcoNwvwA0l5uqSFKcd4TNLLec+gIOleFZliO6/NYElfziuf0fq5nezLAe1vVionC+uP/gwcL6n1KeCz6dyT26+R3BSQPHn74RL2GQx8ObWVWZlxsrD+6kHgU8n7aUBDJ47RyDtP5X4GuCd/o6QrJK1QbtGe/5NULyCZ5E/SNUndIEmLlFsX5M7WsxVJZym35smzkm5tneQwmabkeUn/mXxu6+ednhx3VbLfoZ3ok1lBThbWXzUCdcm8WGPJTaq4v34JfEK5aajrgLtaN0g6h9z0KCcDNcBJkj5Bbp6mF5OFca5Imo8jt97HscBo4ONJXLcDFySL4QwAvpTUfw/4NHAacHhePJcDlybzdJ0GvNGJPpkV5GRh/VIyZfdIcmcVSzp5mD3Af5KbkPE9EfFS3rZzktdTvDM1+1FFjrM8IlqS6VZWJXFVA80R8ULS5gfAJ5LjNEfE+sjd9/6jvOM8DlyXzMM1OCJ2d7JfZu04WVh/thi4ls4NQbVqJLdi4N1t6gV8MzmDqImIqoi4pcgx3sx7v4fcWURH07kXfDgqIhYAnyc3I+sTkspm7RDr/ZwsrD+7ldwEjgeynOpSctNLt004DwOXSBoEIGmYpA8CfyI38WKa54GRkqqS8t8Bv07qR0k6Mqnfu4a6pCOT9R7+ldxswk4W1mWcLKzfSoZ+vlNk8/Q200hXFmqULBl7bURsb1P/C3Kz+S6T9CywCDg0Iv4IPC7pubwL3IWO+1dgBvCTZP+3gX9L6mcCP08ucG/K2+0fkuM+Te56xYOl/H8wK4Wn+zAzs1Q+szAzs1ROFmZmlsrJwszMUjlZmJlZKicLMzNL5WRhZmapnCzMzCzV/wcAFjh29FLAawAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_tra_accs = [0.7446, 0.7132, 0.7452, 0.7447, 0.7507, 0.7503]\n",
    "best_val_accs = [0.7441, 0.7144, 0.7445, 0.7439, 0.7497, 0.7489]\n",
    "best_test_accs = [0.744, 0.713, 0.745, 0.745, 0.751, 0.751]\n",
    "x = np.arange(len(best_tra_accs))\n",
    "plt.bar(x, best_test_accs, width=0.7, color=sns.color_palette('deep'))\n",
    "for i, v in enumerate(best_test_accs):\n",
    "    plt.text(x[i] - 0.28, v + 0.002, str(v))\n",
    "plt.xticks(x, ('LR_SD', 'LR_SGD', 'LS', 'RR', 'LG', 'RLG'))\n",
    "plt.ylim(bottom=0.7, top=0.76)\n",
    "plt.xlabel('ML Methods')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNN - without preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change y label from (1, -1) to (1, 0)\n",
    "nn_x_train = nor_x_train[:, 1:]\n",
    "nn_x_val = nor_x_val[:, 1:]\n",
    "nn_x_test = nor_x_test[:, 1:]\n",
    "nn_y_train = np.where(y_train == -1, 0, y_train)\n",
    "nn_y_val = np.where(y_val == -1, 0, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00000 - 1.0s - loss: 0.4240 - acc: 0.8028 - val_loss: 0.3910 - val_acc: 0.8220\n",
      "Epoch: 00001 - 1.1s - loss: 0.3861 - acc: 0.8249 - val_loss: 0.3802 - val_acc: 0.8271\n",
      "Saving best model (epoch 2, val_acc: 0.8300)\n",
      "Saving best model (epoch 2, val_loss: 0.3745)\n",
      "Saving best model (epoch 3, val_acc: 0.8317)\n",
      "Saving best model (epoch 3, val_loss: 0.3713)\n",
      "Saving best model (epoch 4, val_loss: 0.3705)\n",
      "Epoch: 00005 - 4.5s - loss: 0.3693 - acc: 0.8333 - val_loss: 0.3736 - val_acc: 0.8301\n",
      "Saving best model (epoch 6, val_acc: 0.8319)\n",
      "Saving best model (epoch 6, val_loss: 0.3694)\n",
      "Saving best model (epoch 7, val_acc: 0.8339)\n",
      "Saving best model (epoch 8, val_acc: 0.8349)\n",
      "Saving best model (epoch 8, val_loss: 0.3661)\n",
      "Saving best model (epoch 10, val_loss: 0.3649)\n",
      "Epoch: 00010 - 5.3s - loss: 0.3639 - acc: 0.8362 - val_loss: 0.3649 - val_acc: 0.8347\n",
      "Saving best model (epoch 12, val_acc: 0.8356)\n",
      "Saving best model (epoch 14, val_acc: 0.8362)\n",
      "Saving best model (epoch 14, val_loss: 0.3638)\n",
      "Epoch: 00015 - 5.5s - loss: 0.3608 - acc: 0.8376 - val_loss: 0.3642 - val_acc: 0.8360\n",
      "Saving best model (epoch 16, val_acc: 0.8371)\n",
      "Saving best model (epoch 17, val_loss: 0.3634)\n",
      "Epoch: 00020 - 5.4s - loss: 0.3591 - acc: 0.8380 - val_loss: 0.3654 - val_acc: 0.8356\n",
      "Saving best model (epoch 25, val_acc: 0.8375)\n",
      "Saving best model (epoch 25, val_loss: 0.3632)\n",
      "Epoch: 00025 - 5.4s - loss: 0.3578 - acc: 0.8392 - val_loss: 0.3632 - val_acc: 0.8375\n",
      "Saving best model (epoch 29, val_loss: 0.3630)\n",
      "Epoch: 00030 - 5.4s - loss: 0.3566 - acc: 0.8398 - val_loss: 0.3659 - val_acc: 0.8358\n",
      "Saving best model (epoch 32, val_acc: 0.8375)\n",
      "Epoch: 00035 - 5.4s - loss: 0.3556 - acc: 0.8399 - val_loss: 0.3649 - val_acc: 0.8359\n",
      "Epoch: 00040 - 5.3s - loss: 0.3547 - acc: 0.8406 - val_loss: 0.3668 - val_acc: 0.8359\n",
      "Epoch: 00045 - 5.2s - loss: 0.3541 - acc: 0.8405 - val_loss: 0.3656 - val_acc: 0.8357\n",
      "Epoch: 00050 - 5.3s - loss: 0.3533 - acc: 0.8412 - val_loss: 0.3646 - val_acc: 0.8364\n",
      "Epoch: 00055 - 5.2s - loss: 0.3527 - acc: 0.8416 - val_loss: 0.3654 - val_acc: 0.8364\n",
      "Epoch: 00060 - 5.4s - loss: 0.3523 - acc: 0.8419 - val_loss: 0.3648 - val_acc: 0.8351\n",
      "Epoch: 00065 - 5.3s - loss: 0.3516 - acc: 0.8420 - val_loss: 0.3652 - val_acc: 0.8351\n",
      "Epoch: 00070 - 5.2s - loss: 0.3511 - acc: 0.8426 - val_loss: 0.3650 - val_acc: 0.8355\n",
      "Epoch: 00075 - 5.4s - loss: 0.3508 - acc: 0.8428 - val_loss: 0.3654 - val_acc: 0.8365\n",
      "Epoch: 00080 - 5.4s - loss: 0.3503 - acc: 0.8435 - val_loss: 0.3678 - val_acc: 0.8349\n",
      "Epoch: 00085 - 5.6s - loss: 0.3501 - acc: 0.8434 - val_loss: 0.3650 - val_acc: 0.8347\n",
      "Epoch: 00090 - 5.9s - loss: 0.3496 - acc: 0.8441 - val_loss: 0.3663 - val_acc: 0.8355\n",
      "Epoch: 00095 - 5.4s - loss: 0.3493 - acc: 0.8437 - val_loss: 0.3655 - val_acc: 0.8344\n"
     ]
    }
   ],
   "source": [
    "# set parameters\n",
    "epochs = 100\n",
    "lr = 0.001\n",
    "batch_size = 250\n",
    "\n",
    "# train model, get model parameters\n",
    "nn_params = train(nn_x_train.T, nn_y_train[np.newaxis,:], nn_x_val.T, nn_y_val[np.newaxis,:],\n",
    "                  NN_ARCHITECTURE, epochs, lr, batch_size, False, 0, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_params = np.load('best_acc_0.npy', allow_pickle=True).item()\n",
    "# nn_params = np.load('best_loss_0.npy', allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep neural network\n",
      "Training Loss: 0.3528 - Training Accuracy: 0.8419\n",
      "Validation Loss: 0.3632 - Validation Accuracy: 0.8375\n"
     ]
    }
   ],
   "source": [
    "# get final output of the model\n",
    "y_train_hat, _ = full_forward_propagation(nn_x_train.T, nn_params, NN_ARCHITECTURE)\n",
    "y_val_hat, _ = full_forward_propagation(nn_x_val.T, nn_params, NN_ARCHITECTURE)\n",
    "\n",
    "# compute loss and accuracy\n",
    "train_loss = compute_nn_loss(y_train_hat, nn_y_train.reshape(nn_y_train.shape[0], 1).T)\n",
    "val_loss = compute_nn_loss(y_val_hat, nn_y_val.reshape(nn_y_val.shape[0], 1).T)\n",
    "\n",
    "train_acc = compute_nn_accuracy(y_train_hat, nn_y_train.reshape(nn_y_train.shape[0], 1).T)\n",
    "val_acc = compute_nn_accuracy(y_val_hat, nn_y_val.reshape(nn_y_val.shape[0], 1).T)\n",
    "\n",
    "print('Deep neural network')\n",
    "print(\"Training Loss: {:.4f} - Training Accuracy: {:.4f}\".format(train_loss, train_acc))\n",
    "print(\"Validation Loss: {:.4f} - Validation Accuracy: {:.4f}\".format(val_loss, val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get prediction of testing data\n",
    "y_test_hat, _ = full_forward_propagation(nn_x_test.T, nn_params, NN_ARCHITECTURE)\n",
    "y_test_pred = np.where(np.squeeze(y_test_hat.T) > 0.5, 1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = 'nn_wo_pre.csv'\n",
    "create_csv_submission(ids_test, y_test_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNN - with preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_0, x_1, x_2, y_0, y_1, y_2 = load_train_data_split(DATA_TRAIN_PATH = '../../../../data/train.csv')\n",
    "test_x_0, test_x_1, test_x_2, id_0, id_1, id_2 = load_test_data_split(DATA_TEST_PATH = '../../../../data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_0, y_val_0, tX_train_0, tX_val_0 = train_val_split(y_0, x_0.values, 0.2)\n",
    "y_train_1, y_val_1, tX_train_1, tX_val_1 = train_val_split(y_1, x_1.values, 0.2)\n",
    "y_train_2, y_val_2, tX_train_2, tX_val_2 = train_val_split(y_2, x_2.values, 0.2)\n",
    "\n",
    "# reshape data\n",
    "def transform(tX, y):\n",
    "    tX = tX.T\n",
    "    y = np.reshape(y, (1, y.shape[0]))\n",
    "    nn_y = np.where(y == -1, 0, y)\n",
    "    return tX, nn_y\n",
    "\n",
    "tX_train_0, y_train_0 = transform(tX_train_0, y_train_0)\n",
    "tX_val_0, y_val_0 = transform(tX_val_0, y_val_0)\n",
    "tX_train_1, y_train_1 = transform(tX_train_1, y_train_1)\n",
    "tX_val_1, y_val_1 = transform(tX_val_1, y_val_1)\n",
    "tX_train_2, y_train_2 = transform(tX_train_2, y_train_2)\n",
    "tX_val_2, y_val_2 = transform(tX_val_2, y_val_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00000 - 1.2s - loss: 0.3651 - acc: 0.8392 - val_loss: 0.3486 - val_acc: 0.8448\n",
      "Epoch: 00001 - 1.2s - loss: 0.3429 - acc: 0.8488 - val_loss: 0.3455 - val_acc: 0.8471\n",
      "Saving best model (epoch 2, val_loss: 0.3446)\n",
      "Saving best model (epoch 4, val_loss: 0.3421)\n",
      "Saving best model (epoch 5, val_acc: 0.8481)\n",
      "Saving best model (epoch 5, val_loss: 0.3410)\n",
      "Epoch: 00005 - 4.9s - loss: 0.3354 - acc: 0.8525 - val_loss: 0.3410 - val_acc: 0.8481\n",
      "Saving best model (epoch 8, val_loss: 0.3408)\n",
      "Epoch: 00010 - 6.3s - loss: 0.3320 - acc: 0.8539 - val_loss: 0.3410 - val_acc: 0.8479\n",
      "Saving best model (epoch 11, val_acc: 0.8490)\n",
      "Saving best model (epoch 11, val_loss: 0.3402)\n",
      "Epoch: 00015 - 5.8s - loss: 0.3299 - acc: 0.8544 - val_loss: 0.3427 - val_acc: 0.8478\n",
      "Saving best model (epoch 20, val_acc: 0.8491)\n",
      "Epoch: 00020 - 5.9s - loss: 0.3280 - acc: 0.8552 - val_loss: 0.3419 - val_acc: 0.8491\n",
      "Epoch: 00025 - 6.0s - loss: 0.3260 - acc: 0.8556 - val_loss: 0.3443 - val_acc: 0.8482\n",
      "Epoch: 00030 - 6.1s - loss: 0.3239 - acc: 0.8566 - val_loss: 0.3426 - val_acc: 0.8472\n",
      "Epoch: 00035 - 5.9s - loss: 0.3224 - acc: 0.8572 - val_loss: 0.3457 - val_acc: 0.8481\n",
      "Epoch: 00040 - 6.0s - loss: 0.3206 - acc: 0.8597 - val_loss: 0.3448 - val_acc: 0.8465\n",
      "Epoch: 00045 - 5.9s - loss: 0.3187 - acc: 0.8597 - val_loss: 0.3478 - val_acc: 0.8453\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "\n",
    "# train model 0, get model parameters\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "nn_architecture_0 = [\n",
    "    {\"input_dim\": 18, \"output_dim\": 64, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 64, \"output_dim\": 64, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 64, \"output_dim\": 1, \"activation\": \"sigmoid\"},\n",
    "]\n",
    "param_0 = train(tX_train_0, y_train_0, tX_val_0, y_val_0, nn_architecture_0, epochs, learning_rate, batch_size, True, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00000 - 0.9s - loss: 0.4546 - acc: 0.7858 - val_loss: 0.4243 - val_acc: 0.8064\n",
      "Epoch: 00001 - 0.9s - loss: 0.4116 - acc: 0.8137 - val_loss: 0.4144 - val_acc: 0.8112\n",
      "Saving best model (epoch 2, val_loss: 0.4112)\n",
      "Saving best model (epoch 3, val_acc: 0.8145)\n",
      "Saving best model (epoch 3, val_loss: 0.4086)\n",
      "Saving best model (epoch 4, val_acc: 0.8147)\n",
      "Saving best model (epoch 4, val_loss: 0.4069)\n",
      "Epoch: 00005 - 3.9s - loss: 0.3936 - acc: 0.8216 - val_loss: 0.4095 - val_acc: 0.8136\n",
      "Saving best model (epoch 6, val_acc: 0.8150)\n",
      "Saving best model (epoch 6, val_loss: 0.4069)\n",
      "Saving best model (epoch 8, val_loss: 0.4055)\n",
      "Saving best model (epoch 9, val_acc: 0.8164)\n",
      "Saving best model (epoch 9, val_loss: 0.4039)\n",
      "Saving best model (epoch 10, val_loss: 0.4037)\n",
      "Epoch: 00010 - 4.9s - loss: 0.3856 - acc: 0.8249 - val_loss: 0.4037 - val_acc: 0.8154\n",
      "Saving best model (epoch 12, val_acc: 0.8164)\n",
      "Saving best model (epoch 12, val_loss: 0.4021)\n",
      "Saving best model (epoch 14, val_acc: 0.8185)\n",
      "Saving best model (epoch 14, val_loss: 0.4003)\n",
      "Epoch: 00015 - 4.7s - loss: 0.3809 - acc: 0.8283 - val_loss: 0.4016 - val_acc: 0.8164\n",
      "Epoch: 00020 - 4.8s - loss: 0.3766 - acc: 0.8304 - val_loss: 0.4044 - val_acc: 0.8157\n",
      "Saving best model (epoch 25, val_acc: 0.8190)\n",
      "Epoch: 00025 - 4.7s - loss: 0.3735 - acc: 0.8322 - val_loss: 0.4065 - val_acc: 0.8190\n",
      "Epoch: 00030 - 4.9s - loss: 0.3704 - acc: 0.8331 - val_loss: 0.4053 - val_acc: 0.8165\n",
      "Epoch: 00035 - 4.8s - loss: 0.3670 - acc: 0.8346 - val_loss: 0.4104 - val_acc: 0.8167\n",
      "Epoch: 00040 - 4.9s - loss: 0.3651 - acc: 0.8358 - val_loss: 0.4142 - val_acc: 0.8129\n",
      "Epoch: 00045 - 4.9s - loss: 0.3622 - acc: 0.8372 - val_loss: 0.4151 - val_acc: 0.8165\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "\n",
    "# train model 1, get model parameters\n",
    "nn_architecture_1 = [\n",
    "    {\"input_dim\": 21, \"output_dim\": 64, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 64, \"output_dim\": 64, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 64, \"output_dim\": 1, \"activation\": \"sigmoid\"},\n",
    "]\n",
    "param_1 = train(tX_train_1, y_train_1, tX_val_1, y_val_1, nn_architecture_1, epochs, learning_rate, batch_size, True, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00000 - 2.7s - loss: 0.3989 - acc: 0.8224 - val_loss: 0.3749 - val_acc: 0.8384\n",
      "Epoch: 00001 - 2.4s - loss: 0.3700 - acc: 0.8388 - val_loss: 0.3673 - val_acc: 0.8391\n",
      "Saving best model (epoch 3, val_loss: 0.3637)\n",
      "Saving best model (epoch 5, val_acc: 0.8440)\n",
      "Saving best model (epoch 5, val_loss: 0.3611)\n",
      "Epoch: 00005 - 11.2s - loss: 0.3478 - acc: 0.8474 - val_loss: 0.3611 - val_acc: 0.8440\n",
      "Saving best model (epoch 6, val_acc: 0.8442)\n",
      "Saving best model (epoch 6, val_loss: 0.3593)\n",
      "Saving best model (epoch 8, val_loss: 0.3547)\n",
      "Saving best model (epoch 10, val_acc: 0.8451)\n",
      "Epoch: 00010 - 16.5s - loss: 0.3396 - acc: 0.8517 - val_loss: 0.3576 - val_acc: 0.8451\n",
      "Saving best model (epoch 12, val_acc: 0.8459)\n",
      "Epoch: 00015 - 15.7s - loss: 0.3342 - acc: 0.8540 - val_loss: 0.3695 - val_acc: 0.8442\n",
      "Epoch: 00020 - 15.7s - loss: 0.3286 - acc: 0.8559 - val_loss: 0.3764 - val_acc: 0.8424\n",
      "Epoch: 00025 - 16.7s - loss: 0.3269 - acc: 0.8575 - val_loss: 0.3713 - val_acc: 0.8426\n",
      "Epoch: 00030 - 16.5s - loss: 0.3227 - acc: 0.8588 - val_loss: 0.3740 - val_acc: 0.8432\n",
      "Epoch: 00035 - 15.8s - loss: 0.3209 - acc: 0.8603 - val_loss: 0.3723 - val_acc: 0.8408\n",
      "Epoch: 00040 - 15.5s - loss: 0.3187 - acc: 0.8610 - val_loss: 0.3829 - val_acc: 0.8409\n",
      "Epoch: 00045 - 16.8s - loss: 0.3172 - acc: 0.8601 - val_loss: 0.3816 - val_acc: 0.8378\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "learning_rate = 0.005\n",
    "batch_size = 16\n",
    "\n",
    "# train model 2, get model parameters\n",
    "nn_architecture_2 = [\n",
    "    {\"input_dim\": 29, \"output_dim\": 64, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 64, \"output_dim\": 64, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 64, \"output_dim\": 1, \"activation\": \"sigmoid\"},\n",
    "]\n",
    "param_2 = train(tX_train_2, y_train_2, tX_val_2, y_val_2, nn_architecture_2, epochs, learning_rate, batch_size, True, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_0 = np.load('best_acc_0.npy', allow_pickle=True).item()\n",
    "param_1 = np.load('best_acc_1.npy', allow_pickle=True).item()\n",
    "param_2 = np.load('best_acc_2.npy', allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep neural network - jetnum = 0\n",
      "Training Loss: 0.3251 - Training Accuracy: 0.8556\n",
      "Validation Loss: 0.3419 - Validation Accuracy: 0.8491\n",
      "\n",
      " Deep neural network - jetnum = 1\n",
      "Training Loss: 0.3715 - Training Accuracy: 0.8325\n",
      "Validation Loss: 0.4065 - Validation Accuracy: 0.8190\n",
      "\n",
      " Deep neural network - jetnum = 2\n",
      "Training Loss: 0.3245 - Training Accuracy: 0.8572\n",
      "Validation Loss: 0.3556 - Validation Accuracy: 0.8459\n"
     ]
    }
   ],
   "source": [
    "# get final output of the model - group 0 \n",
    "y_train_hat, _ = full_forward_propagation(tX_train_0, param_0, nn_architecture_0)\n",
    "y_val_hat, _ = full_forward_propagation(tX_val_0, param_0, nn_architecture_0)\n",
    "\n",
    "# compute loss and accuracy\n",
    "train_loss = compute_nn_loss(y_train_hat, y_train_0)\n",
    "val_loss = compute_nn_loss(y_val_hat, y_val_0)\n",
    "\n",
    "train_acc = compute_nn_accuracy(y_train_hat, y_train_0)\n",
    "val_acc = compute_nn_accuracy(y_val_hat, y_val_0)\n",
    "\n",
    "print('Deep neural network - jetnum = 0')\n",
    "print(\"Training Loss: {:.4f} - Training Accuracy: {:.4f}\".format(train_loss, train_acc))\n",
    "print(\"Validation Loss: {:.4f} - Validation Accuracy: {:.4f}\".format(val_loss, val_acc))\n",
    "\n",
    "# get final output of the model\n",
    "y_train_hat, _ = full_forward_propagation(tX_train_1, param_1, nn_architecture_1)\n",
    "y_val_hat, _ = full_forward_propagation(tX_val_1, param_1, nn_architecture_1)\n",
    "\n",
    "# compute loss and accuracy - group 1\n",
    "train_loss = compute_nn_loss(y_train_hat, y_train_1)\n",
    "val_loss = compute_nn_loss(y_val_hat, y_val_1)\n",
    "\n",
    "train_acc = compute_nn_accuracy(y_train_hat, y_train_1)\n",
    "val_acc = compute_nn_accuracy(y_val_hat, y_val_1)\n",
    "\n",
    "print('\\n Deep neural network - jetnum = 1')\n",
    "print(\"Training Loss: {:.4f} - Training Accuracy: {:.4f}\".format(train_loss, train_acc))\n",
    "print(\"Validation Loss: {:.4f} - Validation Accuracy: {:.4f}\".format(val_loss, val_acc))\n",
    "\n",
    "# get final output of the model\n",
    "y_train_hat, _ = full_forward_propagation(tX_train_2, param_2, nn_architecture_2)\n",
    "y_val_hat, _ = full_forward_propagation(tX_val_2, param_2, nn_architecture_2)\n",
    "\n",
    "# compute loss and accuracy - group 2\n",
    "train_loss = compute_nn_loss(y_train_hat, y_train_2)\n",
    "val_loss = compute_nn_loss(y_val_hat, y_val_2)\n",
    "\n",
    "train_acc = compute_nn_accuracy(y_train_hat, y_train_2)\n",
    "val_acc = compute_nn_accuracy(y_val_hat, y_val_2)\n",
    "\n",
    "print('\\n Deep neural network - jetnum = 2')\n",
    "print(\"Training Loss: {:.4f} - Training Accuracy: {:.4f}\".format(train_loss, train_acc))\n",
    "print(\"Validation Loss: {:.4f} - Validation Accuracy: {:.4f}\".format(val_loss, val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get prediction of testing data\n",
    "y_test_hat_0, _ = full_forward_propagation(test_x_0.values.T, param_0, nn_architecture_0)\n",
    "y_test_pred_0 = np.where(np.squeeze(y_test_hat_0.T) > 0.5, 1, -1)\n",
    "y_test_hat_1, _ = full_forward_propagation(test_x_1.values.T, param_1, nn_architecture_1)\n",
    "y_test_pred_1 = np.where(np.squeeze(y_test_hat_1.T) > 0.5, 1, -1)\n",
    "y_test_hat_2, _ = full_forward_propagation(test_x_2.T, param_2, nn_architecture_2)\n",
    "y_test_pred_2 = np.where(np.squeeze(y_test_hat_2.T) > 0.5, 1, -1)\n",
    "\n",
    "# concatenate three predicted results\n",
    "y_test_pred_tmp = np.concatenate((y_test_pred_0, y_test_pred_1), axis = 0)\n",
    "y_test_pred = np.concatenate((y_test_pred_tmp, y_test_pred_2), axis = 0)\n",
    "\n",
    "# concatenate the indexs of three group\n",
    "ids = np.concatenate((id_0, id_1), axis = 0)\n",
    "ids = np.concatenate((ids, id_2), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = 'nn_w_pre.csv'\n",
    "create_csv_submission(ids, y_test_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
